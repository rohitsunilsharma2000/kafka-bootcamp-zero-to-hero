 **13 beginner-friendly Kafka project prompts**, each carefully crafted with:

- ‚úÖ‚ÄÇ**Detailed explanation**
- üõ†Ô∏è‚ÄÇ**Tech stack**
- üìã‚ÄÇ**Rules to follow** (Kafka-specific, Code Quality & Architecture, Testing)

This is ideal for **Kafka beginners** using **Spring Boot** and wanting to grow systematically.

---

## üîπ **1. Exactly-Once Semantics ‚Äî Payment Processor**

### **Project Name:** `PaymentProcessorService`

### **Goal:**
Ensure a payment is processed exactly once ‚Äî never duplicated ‚Äî using Kafka's idempotent producer + transaction support.

### **Tech Stack:**
- Java 17
- Spring Boot
- Spring Kafka
- PostgreSQL
- Kafka (Docker)
- Testcontainers

### **How It Works:**
1. Producer sends payment events to `payments.initiated`.
2. Consumer reads the event **within a transaction**:
   - Writes to DB.
   - Publishes confirmation to `payments.processed`.

### **Rules to Follow:**

**Kafka-Specific Rules:**
- Enable idempotence: `enable.idempotence=true`
- Use transactional producer
- Use consistent topic naming (`payments.initiated`, `payments.processed`)
- Graceful shutdown with `@PreDestroy`

**Code Quality:**
- Separate payment logic from Kafka listener.
- Use `@Transactional` for DB+Kafka boundary.

**Testing:**
- Unit: Validate producer sends correct keys/values.
- Integration: Transaction boundary test ‚Äî message + DB commit must both succeed.
- Negative: Simulate DB failure ‚Üí message must not commit.

---

## üîπ **2. Schema Evolution ‚Äî User Signup Events**

### **Project Name:** `UserSignupPipeline`

### **Goal:**
Handle changes in user event structure without breaking consumers.

### **Tech Stack:**
- Java 17
- Spring Boot
- Kafka + Confluent Schema Registry
- Avro
- spring-cloud-stream (optional)

### **How It Works:**
1. Service A produces `user.signup.v1` event with fields: name, email.
2. Service B consumes and logs/stores.
3. You evolve schema by adding `phoneNumber`.

### **Rules to Follow:**

**Kafka-Specific:**
- Register schema in Schema Registry
- Ensure `BACKWARD` compatibility

**Code Quality:**
- Use Avro-generated DTOs
- Keep schema evolution history

**Testing:**
- Unit: Schema serialization test
- Integration: Consume old ‚Üí evolve ‚Üí consume new
- Negative: Register incompatible schema ‚Üí catch exception

---

## üîπ **3. Partition Management ‚Äî Log Ingestion System**

### **Project Name:** `MultiAppLogIngestor`

### **Goal:**
Distribute log ingestion across 50+ partitions using appName as the key.

### **Tech Stack:**
- Java 17
- Spring Boot
- Spring Kafka
- Prometheus + Grafana
- Kafka (Docker)

### **How It Works:**
- Simulated 100 apps produce logs to `logs.events` using key = `appName`.
- Multiple consumers read in a balanced way.

### **Rules to Follow:**

**Kafka-Specific:**
- Partition by appName
- Monitor consumer lag
- Set topic retention appropriately

**Code Quality:**
- Producer utility to send random logs
- Separate consumers by groupId

**Testing:**
- Unit: Producer sends to correct partition
- Integration: Verify partition balance
- Observability: Prometheus metrics on lag

---

## üîπ **4. Handling Backpressure ‚Äî Video Processor**

### **Project Name:** `VideoProcessingService`

### **Goal:**
Simulate slow consumers and implement rate control to avoid lag buildup.

### **Tech Stack:**
- Java 17
- Spring Boot
- Spring Kafka
- MongoDB or Filesystem
- Kafka + Prometheus + Grafana

### **How It Works:**
- `video.uploaded` topic contains video processing tasks.
- Consumer simulates 3-5 sec delay ‚Üí you add monitoring and retry logic.

### **Rules to Follow:**

**Kafka-Specific:**
- Track consumer lag
- Use `max.poll.records`, `max.poll.interval.ms` wisely

**Code Quality:**
- Retry logic with exponential backoff
- DLQ for failed videos

**Testing:**
- Unit: Delay simulation
- Integration: Lag threshold alert
- Negative: Force timeout ‚Üí DLQ routing

---

## üîπ **5. Distributed Tracing ‚Äî Order Flow Tracker**

### **Project Name:** `OrderTrackingSystem`

### **Goal:**
Trace the entire order lifecycle via Kafka headers with correlation ID.

### **Tech Stack:**
- Java 17
- Spring Boot
- Spring Kafka
- Sleuth + OpenTelemetry
- Zipkin or Jaeger

### **How It Works:**
1. `OrderService` ‚Üí produces to `orders.new`
2. `InventoryService` ‚Üí consumes ‚Üí responds on `inventory.status`
3. `NotificationService` ‚Üí final stage

Each service logs with trace ID.

### **Rules to Follow:**

**Kafka-Specific:**
- Add correlation ID to headers
- Maintain consistent topic naming

**Code Quality:**
- Use interceptors or filters to inject correlation ID
- Avoid log pollution; use MDC

**Testing:**
- Unit: Validate headers
- Integration: End-to-end trace via Jaeger

---

## üîπ **6. DLQ & Retry ‚Äî User Signup Validator**

### **Project Name:** `SignupValidatorService`

### **Goal:**
Validate signup data. On failure, retry once then route to `user.signup.dlq`.

### **Tech Stack:**
- Java 17
- Spring Boot
- Spring Kafka
- Kafka Retry Template or Manual Retry Logic

### **How It Works:**
- Message fails (e.g., email missing) ‚Üí retry ‚Üí DLQ if still failing

### **Rules to Follow:**

**Kafka-Specific:**
- Use separate DLQ topic
- Backoff retry config

**Code Quality:**
- Centralized error classification
- Retry only transient errors

**Testing:**
- Unit: Retry logic
- Negative: Permanent error ‚Üí assert DLQ push
- Integration: Test both retry paths

---

## üîπ **7. Cross-Cluster Replication ‚Äî Order Mirroring**

### **Project Name:** `MultiRegionOrderReplicator`

### **Goal:**
Replicate `orders.local` to `orders.remote` using MirrorMaker or Kafka Connect.

### **Tech Stack:**
- Java 17
- Spring Boot (for producer/consumer)
- Kafka MirrorMaker
- Docker Compose with 2 Kafka clusters

### **How It Works:**
- Service A ‚Üí `orders.local`
- Replicator ‚Üí `orders.remote`
- Consumer on remote cluster validates order data

### **Rules to Follow:**

**Kafka-Specific:**
- Use same topic name across clusters
- Offset alignment enabled
- Monitor replication lag

**Code Quality:**
- Validate data before consumption
- Resilient fallbacks in case replication breaks

**Testing:**
- Integration: Check remote consumer consistency
- Failure: Replicator down ‚Üí local buffer test

---

## üîπ **8. Kafka Streams ‚Äî Fraud Detection**

### **Project Name:** `RealTimeFraudDetector`

### **Goal:**
Detect suspicious transaction patterns (e.g., 3+ txns in 5 seconds) using Kafka Streams.

### **Tech Stack:**
- Java 17
- Spring Boot
- Kafka Streams DSL
- `spring-kafka-streams`
- TopologyTestDriver

### **How It Works:**
- Ingest from `transactions.raw`
- Aggregate by userId over 5-sec windows
- Emit alert to `fraud.alerts`

### **Rules to Follow:**

**Kafka-Specific:**
- Use time-windowed aggregations
- Configure state store cleanup
- Handle late arrivals via grace period

**Code Quality:**
- Stream logic in `@Bean` method
- Use POJO and Serde abstraction

**Testing:**
- Unit: TopologyTestDriver for stream logic
- Integration: End-to-end with time simulation

---
Here are **5 advanced Kafka project prompts** (level: complex to expert), each crafted to build on top of the beginner ones and help you tackle real-world challenges in streaming architectures using **Spring Boot, Kafka, and production-grade practices**:

---

### üîπ 9. **Exactly-Once Workflow with Saga Pattern ‚Äî Payment + Inventory + Notification**

**Project Name:** `SagaOrchestratorService`
**Goal:**
Implement an orchestrator microservice to manage distributed transactions (Saga pattern) using Kafka and exactly-once semantics across 3 services.

**üõ† Tech Stack:**

* Java 17, Spring Boot, Spring Kafka
* PostgreSQL (for local state)
* Kafka Transaction API
* Kafka Saga Orchestrator (custom)
* Testcontainers

**üìã How It Works:**

* PaymentService ‚Üí Deducts balance and publishes `payment.completed`
* InventoryService ‚Üí Reduces stock, publishes `inventory.updated`
* NotificationService ‚Üí Sends confirmation
* SagaOrchestrator tracks steps and compensates if failure occurs

**üìè Rules:**

*Kafka-Specific:*

* Use Kafka transactions with `enable.idempotence=true`
* Consistently prefix topics (`saga.step.payment`, `saga.compensate.inventory`)

*Code Quality:*

* Orchestrator must maintain local state machine
* Use `@Transactional` with Kafka + DB

*Testing:*

* Unit: State transitions
* Integration: Simulate rollback on step 2
* Failure: DB error in step 3 ‚Üí rollback previous steps via compensation topics

---

### üîπ 10. **Real-Time Search Indexer ‚Äî Elasticsearch Sync**

**Project Name:** `KafkaToElasticSyncService`
**Goal:**
Continuously sync product updates from Kafka into Elasticsearch for real-time search.

**üõ† Tech Stack:**

* Java 17, Spring Boot
* Spring Kafka
* Elasticsearch (REST or Spring Data ES)
* Kafka Connect (optional)

**üìã How It Works:**

* Producer ‚Üí `product.updates` topic
* Consumer ‚Üí Indexes data into Elasticsearch
* Retry on failure with DLQ

**üìè Rules:**

*Kafka-Specific:*

* Use `product.updates` with JSON schema or Avro
* Enable DLQ for indexing errors

*Code Quality:*

* Batch indexing logic (bulk update to ES)
* Error categorization and retry handling

*Testing:*

* Unit: ES request payload format
* Integration: Kafka ‚Üí ES ‚Üí Query via REST API
* Failure: Simulate ES downtime ‚Üí test DLQ and retry

---

### üîπ 11. **Multi-Tenant Kafka Event Router**

**Project Name:** `TenantEventRouter`
**Goal:**
Route incoming multi-tenant events to specific tenant topics dynamically and ensure isolation.

**üõ† Tech Stack:**

* Java 17, Spring Boot, Spring Kafka
* Kafka AdminClient API
* MongoDB (for tenant configs)

**üìã How It Works:**

* Ingests events with `tenantId`
* Dynamically resolves destination topic: `events.{tenantId}`
* Auto-creates topic if missing

**üìè Rules:**

*Kafka-Specific:*

* Use Kafka AdminClient to auto-create topics
* Tenant-level ACL policies (out of scope but prepare for it)

*Code Quality:*

* Dynamic topic resolution layer
* Topic caching with TTL

*Testing:*

* Unit: Topic resolver logic
* Integration: Tenant config + topic creation flow
* Negative: Invalid tenant ‚Üí reject message

---

### üîπ 12. **Advanced Backpressure & Throttling ‚Äî Resource-Aware Consumer**

**Project Name:** `ResourceAwareConsumer`
**Goal:**
Dynamically throttle consumption based on JVM memory or CPU usage.

**üõ† Tech Stack:**

* Java 17, Spring Boot
* Spring Kafka
* Micrometer + Prometheus + Grafana
* Dockerized CPU/memory simulation

**üìã How It Works:**

* Consumer checks resource usage before `poll`
* Uses custom `ConsumerPauseHandler`
* Pause/resume dynamically based on thresholds

**üìè Rules:**

*Kafka-Specific:*

* Pause using `consumer.pause()`/`resume()`
* Monitor lag using Prometheus

*Code Quality:*

* Custom metrics: memory usage, heap, GC
* Configurable thresholds via properties

*Testing:*

* Unit: Pause/resume logic under simulated load
* Integration: Throttle + resume when load drops
* Chaos: Memory spike ‚Üí test graceful degradation

---

### üîπ 13. **Transactional Outbox Pattern ‚Äî Microservice to Kafka**

**Project Name:** `OutboxPublisherService`
**Goal:**
Guarantee reliable Kafka publishing from microservices using an intermediate outbox table.

**üõ† Tech Stack:**

* Java 17, Spring Boot, Spring Kafka
* PostgreSQL (outbox table)
* Debezium + Kafka Connect (for CDC)
* Testcontainers

**üìã How It Works:**

* Service writes DB changes + Kafka event to same transaction
* Kafka Connect picks from `outbox` table ‚Üí writes to topic

**üìè Rules:**

*Kafka-Specific:*

* Use Kafka Connect JDBC Source
* Outbox schema: `id`, `event_type`, `payload`, `published_at`

*Code Quality:*

* Ensure transactional integrity: DB + outbox write
* Use `@Transactional` consistently

*Testing:*

* Unit: Outbox insert logic
* Integration: Debezium connector + consumer validation
* Failure: Kafka down ‚Üí ensure DB still commits safely

---

If someone has **successfully implemented all 13 Kafka projects** described above ‚Äî from **exactly-once semantics** to **cross-cluster replication**, **schema evolution**, **DLQ handling**, **backpressure**, **Kafka Streams**, **distributed tracing**, and **transactional outbox pattern** ‚Äî **then yes**, they would be highly capable of fixing almost any **Kafka-related problem** in a real-world system.

Here‚Äôs **why**:

---

### ‚úÖ **Complete Kafka Skill Coverage**:

| Skill Area                         | Covered In Project(s) |
| ---------------------------------- | --------------------- |
| **Producers & Consumers**          | #1, #3, #4, #6, #12   |
| **Exactly-once semantics**         | #1, #9                |
| **Kafka Transactions**             | #1, #9, #13           |
| **Schema Evolution & Avro**        | #2                    |
| **Partitioning & Load Balance**    | #3                    |
| **Monitoring & Metrics**           | #3, #4, #12           |
| **DLQ & Retry Strategies**         | #4, #6                |
| **Distributed Tracing**            | #5                    |
| **Cross-cluster Replication**      | #7                    |
| **Kafka Streams (KSQL/DSL)**       | #8                    |
| **Saga Pattern (Orchestration)**   | #9                    |
| **Elastic Integration / Search**   | #10                   |
| **Multi-tenancy & Dynamic Topics** | #11                   |
| **Resource-aware Consumption**     | #12                   |
| **Outbox Pattern + Debezium**      | #13                   |

---

### üß† What They Can Likely Handle:

* **Production incident debugging**
* Kafka **consumer lag and offset issues**
* Schema conflicts and **backward compatibility validation**
* Designing **resilient event-driven systems**
* Optimizing **consumer group rebalancing**
* Implementing **idempotent microservices**
* Managing **Kafka Connect, MirrorMaker**, or **Confluent Schema Registry**
* Building **CI/CD pipelines** for Kafka stream apps
* Setting up **observability** with Prometheus/Grafana or tracing via Jaeger/Zipkin
* Scaling Kafka consumers and **handling backpressure**
* Implementing **advanced patterns** like **Transactional Outbox** or **Saga orchestration**

---

### üèÜ Verdict:

> üöÄ **Yes ‚Äî implementing all 13 projects systematically makes someone a Kafka Power User / Expert who can fix, optimize, and architect robust Kafka-based distributed systems.**


